\documentclass[12pt]{article}

%Preamble

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsrefs}
\usepackage{amsfonts}
%\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{mathtools}
%\usepackage{stmaryrd}
%\usepackage[all]{xy}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{verbatim} %% includes comment environment
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\crefformat{equation}{~(#2#1#3)}
\usepackage{caption, subcaption}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{fullpage} %%smaller margins
\usepackage[all,arc]{xy}
\usepackage{mathrsfs}

%% Sectioning, Header / Footer, ToC
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{tocloft}


%% Optional Code Snippets

%\usepackage{minted} %Render Code.
%% Must add (% !TEX option = --shell-escape) to top of page.
%\usemintedstyle{colorful}

\hypersetup{
    linktoc=all,     %set to all if you want both sections and subsections linked
}

\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bbC}{\mathbb{C}}

\newcommand{\abs}[1]{ \left| #1 \right| }
\newcommand{\diff}[2]{\frac{d #1}{d #2}}
\newcommand{\infsum}[1]{\sum_{#1}^{\infty}}
\newcommand{\norm}[1]{ \left|\left| #1 \right|\right| }
\newcommand{\eval}[1]{ \left. #1 \right| }

\renewcommand{\phi}{\varphi}

%--------Theorem Environments--------
%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\numberwithin{equation}{section}

\bibliographystyle{plain}

%% Sectioning Aesthetics
\titleformat{\section}
{\normalfont\LARGE\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}
{\normalfont\Large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}[runin]
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titleformat{\subparagraph}[runin]
{\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{}


%% Header Aesthetics
\pagestyle{fancy}

\setlength{\headheight}{16pt}
\setlength{\headsep}{0.3in}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\renewcommand{\contentsname}{\hfill\bfseries\Large Table of Contents\hfill}
\renewcommand{\sectionmark}[1]{\markright{ #1}}

\lhead{\textbf{}} % controls the left corner of the header
%\chead{\fancyplain{}{\rightmark }}
 % controls the center of the header / adds section # to top
\rhead[]{Marlin Figgins} % controls the right corner of the header
\lfoot{Last updated: \today} % controls the left corner of the footer
\cfoot{} % controls the center of the footer
\rfoot{Page~\thepage} % controls the right corner of the footer

\title{\bfseries\huge{AMATH 584A: Applied Linear Algebra}\vspace{-1ex}} \author{\href{marlinfiggins@gmail.com}{\Large{Marlin Figgins}}\vspace{-2ex}}
\date{\large{Oct. 1, 2020}}

\begin{document}

\maketitle

	\section*{\hfill Introduction \hfill}

  \thispagestyle{empty}

  %% Table of Contents Page/
  \newpage
  \tableofcontents
  \thispagestyle{empty}
  \newpage

  %% Set first page after ToC
  \setcounter{page}{1}


  %% Start here.

  \section{Lecture 1: Overview}%
  \label{sec:lecture_1}
 
  This course will be entirely about the problem of $Ax=b$. That is, we're concerning with linear systems. In fact, many problems are of this form. In the age of data science, these variables $A$ and $x$ can get huge quickly. In your typical linear algebra classes, you learn to solve this with Gaussian elimination, but the reality is that this is one of the slowest ways you can solve this problem.  

  \subsection{Matrix Decompositions}%
  \label{sub:matrix_decompositions}

  Matrix decompositions allow us to solve the problem $Ax = b$ much faster. Let's start with the case of complex square matrices $A\in \bbC^{n \times n}$.

  To solve this problem with Gaussian Elimination, the cost would be on the order of $O(n^3)$. This is fine for small matrices, but immagine you're dealing with large matrices and this begins to blow up in computation time rather quickly.

  % Rephrase cost of computation to the number of computations necessary.
  \subsubsection{LU decomposiiton}%
  \label{ssub:lu_decomposiiton}
  
  The $LU$ decomposition allows us to represent our matrix $A$ as 

  \begin{equation}
    A = LU
  \end{equation}
  
  where $L$ is lower triangular and $U$ is upper triangular. Our problem becomes

  \begin{align}
    A x= b \\
    LUx = b \\
    Ux = y \\
    Ly = b 
  \end{align}
  
  This allows us to use forward and back substituion individually which are of order $O(n^2)$ to solve this probelm. This $LU$ decompoisition already gives a saving of order of $n$. This is all well and good, but what does it take to get an $LU$ decomposition?

  \subsubsection{QR decomposition}%
  \label{ssub:qr_decomposition}
  
  We want to express our matrix $A$ in the form

  \begin{equation}
    A = QR
  \end{equation}
  
  where $Q$ is a unitary matrix and $R$ is upper triangular. Solving $Ax=b$ with this decomposiiton gives us,
  
  \begin{align}
    QRx=b\\
    Rx = y\\
    Qy = b \\
    Q^T [Qy = b] \\
    y = Q^T b
  \end{align}

\subsubsection{Eigenvalue Decomposition}%
\label{ssub:eigenvalue_decomposition}

We can write the eigenvale decomposition as 

\begin{equation}
  A = V \Lambda V^{-1}
\end{equation}

Using this to solve $Ax=b$, we get that

\begin{align}
  V^{-1} [ V \Lambda V^{-1} x = b]\\
  \Lambda y = V^{-1} b
 \end{align}

 Since $\Lambda$ is diagonal, the answer is very clear here.

\subsubsection{Singular Value Decomposition}%
 \label{ssub:singular_value_decomposition}
 
The singular value decomposition is one of the most important decomposition algorithms. We decompose $A$ as

\begin{equation}
  A = U \Sigma V^{*} 
\end{equation}

Solving $Ax=b$,

\begin{align}
  U\Sigma V^{*} x = b \\
\Sigma V^* x = U^* b \\
\Sigma \hat{x} = \hat{b}
\end{align}

\section{Lecture 2}%
\label{sec:lecture_2}

In reality, we're often dealing with systems and matrices which are not perfectly square. Many problems are not perfectly square. In reality, very few are. The rest of these problems fall into two general categories of systems which we call underdetermined and overdetermined systems. When it comes to solving $Ax=b$ for these problems, the question is ill-posed. Though these systems may have no solutions or infinitely many solutions, most software will still be able to solve the problem $Ax=b$, how is this done?
%TODO: More exposition on what it means for a system to be OD or UD.
\paragraph{Underdetermined systems $(m < n)$.}

These systems fundamentally have infinitely many solutions, so $Ax=b$ is instead posed as an optimization problem
\begin{equation}
  \min_x \norm{x}_2 \text{ such that } Ax = b.
\end{equation}
In this case, the minimization of the $L^2$ norm acts as a regularizer for our desired solution $x$.

\paragraph{Overdetermined systsems $(m>n)$.}

Due to the abundance of constraints, satisfying $Ax=b$ is technically impossible. In this case, we attempt to find the closest possible solution i.e.

\begin{equation}
  \min_x \norm{Ax + b} + \lambda \norm{x}_2.
\end{equation}

Here the $L^2$ norm acts as a regularizer for our solution $x$. We use the parameter $\lambda$ as a hyperparameter which determines the relative importance of the regularizer $\lambda$. Indeed, there are several different ways to do this regularization such as using the $L^1$ norm.

\begin{equation}
  \min_x \norm{Ax + b} + \lambda_1 \norm{x}_1 + \lambda_2 \norm{x}_2.
\end{equation}

\subsection{Norms}%
\label{sub:norms}

In short, a norm is just a way of quantifying distance. In particular, the two most interesting norms that we'll cover are the $L^2$ and $L^1$ norms.

\paragraph{$L^2$ norm}%
\label{par:_l_2_norm}
We can define the $L^2$ norm of a vector $x$ as 
\begin{equation}
  \norm{x}_2 = \sqrt{\abs{x_1}^2 + \abs{x_2}^2}
\end{equation}

Notice, this is the distance that we're used to in most geometric contexts.

\paragraph{$L^1$ norm}%
\label{par:_l_1_norm}

There are also various other norms such as the $L^1$ norm which we denote as $\norm{\cdot}_1$
\begin{equation}
  \norm{x}_1 = \abs{x_1} + \abs{x_2}
\end{equation}

In applications, the $L^1$ norm tends to promote sparsity in solutions.

\paragraph{Other norms}%
\label{par:other_norms} 

In general, we can compute the $L^p$ norm of a vector $x$ as 
\begin{equation}
  \norm{x}_p = (\abs{x_1}^p + \abs{x_2}^p)^{1/p}.
\end{equation}

There are also the $L^\infty$ and $L^0$ norms.

\subsection{Nullspaces and zero eigenvalues}%
\label{sub:nullspaces}

Consider the problem $A^*y = 0$ where $A^*$ is the adjoint of the matrix $A$. When does $Ax=b$ have a solution?
\begin{align}
  Ax \cdot y &= b \cdot y\\
  x\cdot A^*y &= b\cdot y \\
  b\cdot y &= 0
\end{align}

The Fredholm alternative is the statement that $b$ is not orthogonal to $y$, then $Ax=b$ has no solutions.
% TODO: Double check this. Flesh this out.

Suppose we have the problem $Ax=b$ and a vector $x_0$ with 0 eigenvalue $Ax_0 = 0$. Then we can generate solutions as any vector of the form
\begin{equation}
  x = \xi + \alpha x_0,
\end{equation}
where $\xi$ is a solution. The regularization process is an attempt to avoid this by minimizing across the vectors $x_0$ in the nullspace.
%TODO: Clean up this langauge.

\subsection{Linear Operators}%
\label{sub:linear_operators}

Linear operators are communtative and associative under addition.
\begin{enumerate}[(i)]
  \item \emph{Commutative. (+)} $A + B = B + A$
  \item \emph{Associative. (+)} $A + (B + C) = (A + B) + C$
  \item \emph{Distributive.} $A(B + C) = AB + BC$
  \item \emph{Associative ($\cdot$).} $(AB)C = A(BC)$
\end{enumerate}

Though we have all these algebraic properties, it is important to know that multiplication is not commutative for matrices in general i.e.
\begin{equation}
  BA \neq AB. 
\end{equation}

\end{document}
