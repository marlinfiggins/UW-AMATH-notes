\documentclass[12pt]{article}

%Preamble

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsrefs}
\usepackage{amsfonts}
%\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{mathtools}
%\usepackage{stmaryrd}
%\usepackage[all]{xy}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{verbatim} %% includes comment environment
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\crefformat{equation}{~(#2#1#3)}
\usepackage{caption, subcaption}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{fullpage} %%smaller margins
\usepackage[all,arc]{xy}
\usepackage{mathrsfs}

%% Sectioning, Header / Footer, ToC
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{tocloft}


%% Optional Code Snippets

%\usepackage{minted} %Render Code.
%% Must add (% !TEX option = --shell-escape) to top of page.
%\usemintedstyle{colorful}

\hypersetup{
    linktoc=all,     %set to all if you want both sections and subsections linked
}

\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bbC}{\mathbb{C}}

\newcommand{\abs}[1]{ \left| #1 \right| }
\newcommand{\diff}[2]{\frac{d #1}{d #2}}
\newcommand{\infsum}[1]{\sum_{#1}^{\infty}}
\newcommand{\norm}[1]{ \left|\left| #1 \right|\right| }
\newcommand{\eval}[1]{ \left. #1 \right| }

\renewcommand{\phi}{\varphi}
\renewcommand{\vec}[1]{\mathbf{#1}}

%--------Theorem Environments--------
%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\numberwithin{equation}{section}

\bibliographystyle{plain}

%% Sectioning Aesthetics
\titleformat{\section}
{\normalfont\LARGE\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}
{\normalfont\Large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}
\titleformat{\paragraph}[runin]
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titleformat{\subparagraph}[runin]
{\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{}


%% Header Aesthetics
\pagestyle{fancy}

\setlength{\headheight}{16pt}
\setlength{\headsep}{0.3in}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\renewcommand{\contentsname}{\hfill\bfseries\Large Table of Contents\hfill}
\renewcommand{\sectionmark}[1]{\markright{ #1}}

\lhead{\textbf{}} % controls the left corner of the header
%\chead{\fancyplain{}{\rightmark }}
 % controls the center of the header / adds section # to top
\rhead[]{Marlin Figgins} % controls the right corner of the header
\lfoot{Last updated: \today} % controls the left corner of the footer
\cfoot{} % controls the center of the footer
\rfoot{Page~\thepage} % controls the right corner of the footer

\title{\bfseries\huge{AMATH 584A: Applied Linear Algebra}\vspace{-1ex}} \author{\href{marlinfiggins@gmail.com}{\Large{Marlin Figgins}}\vspace{-2ex}}
\date{\large{Oct. 1, 2020}}

\begin{document}

\maketitle

	\section*{\hfill Introduction \hfill}

  \thispagestyle{empty}

  %% Table of Contents Page/
  \newpage
  \tableofcontents
  \thispagestyle{empty}
  \newpage

  %% Set first page after ToC
  \setcounter{page}{1}


  %% Start here.
  \section{Overview: The problem Ax=b}
  This course will be almost entirely about the problem of $\vec{Ax}=\vec{b}$. That is, we're concerning with linear systems. In fact, many problems are of this form. In the age of data science, these matrix $\vec{A}$ and vector $\vec{x}$ can get huge quickly. 
  
  \subsection{Matrix Decompositions}%
  \label{sub:matrix_decompositions}

  In what follows, let's assume we are given a complex matrix $\vec{A}\in \bbC^{n \times n}$ and a vector $\vec{b}$. Suppose that we're given the problem
  
  \begin{equation}
    \vec{Ax} = \vec{b}.
  \end{equation}

In your typical linear algebra classes, you learn to solve this with Gaussian elimination, but the reality is that this is one of the slowest ways you can solve this problem. To solve this problem with Gaussian Elimination, the cost would be on the order of $O(n^3)$. This is fine for small matrices, but immagine you're dealing with large matrices and this begins to blow up in computation time rather quickly. Matrix decompositions allow us to solve the problem $\vec{Ax} = \vec{b}$ much faster. We'll start with a simple overview of several matrix decompositions such as the $\vec{LU}$, $\vec{QR}$, eigenvalue, and singular value decompositions.

  %TODO: Rephrase cost of computation to the number of computations necessary.
  \subsubsection{LU decomposiiton}%
  \label{ssub:lu_decomposiiton}
  
  The $\vec{LU}$ decomposition allows us to represent our matrix $\vec{A}$ as 

  \begin{equation}
    \vec{A} = \vec{LU}
  \end{equation}
  
  where $\vec{L}$ is a lower triangular matrix and $\vec{U}$ is upper triangular. Our problem becomes

  \begin{align}
    A x= b \\
    LUx = b \\
    Ux = y \\
    Ly = b 
  \end{align}
  
  This allows us to use forward and back substituion individually which are of order $O(n^2)$ to solve this probelm. This $\vec{LU}$ decompoisition already gives a saving of order of $n$. This is all well and good, but what does it take to get an $\vec{LU}$ decomposition?


  \subsubsection{QR decomposition}%
  \label{ssub:qr_decomposition}
  
  We want to express our matrix $\vec{A}$ in the form

  \begin{equation}
    \vec{A} = \vec{QR}
  \end{equation}
  
  where $\vec{Q}$ is a unitary matrix and $\vec{R}$ is upper triangular. Solving $vec{A}x=b$ with this decomposiiton gives us,
  
  \begin{align}
    QRx=b\\
    Rx = y\\
    Qy = b \\
    Q^T [Qy = b] \\
    y = Q^T b
  \end{align}

\subsubsection{Eigenvalue Decomposition}%
\label{ssub:eigenvalue_decomposition}

We can write the eigenvalue decomposition as 

\begin{equation}
  \vec{A} = \vec{V} \vec{\Lambda} \vec{V}^{-1}
\end{equation}

Using this to solve $\vec{Ax}=\vec{b}$, we get that

\begin{align}
  V^{-1} [ V \Lambda V^{-1} x = b]\\
  \Lambda y = V^{-1} b
 \end{align}

 Since $\vec{\Lambda}$ is diagonal, the answer is very clear here.

\subsubsection{Singular Value Decomposition}%
 \label{ssub:singular_value_decomposition}
 
The singular value decomposition is one of the most important decomposition algorithms. We decompose $\vec{A}$ as

\begin{equation}
  \vec{A} = \vec{U \Sigma V}^{*}. 
\end{equation}

%TODO: Talk a bit more about this decomposition and its usefulness

Solving $\vec{A}x=b$,

\begin{align}
  U\Sigma V^{*} x = b \\
\Sigma V^* x = U^* b \\
\Sigma \hat{x} = \hat{b}
\end{align}

%TODO: Take notes on the video lectures

\subsection{Under and over determined systems}%
\label{sub:under_and_over_determined_systems}


In reality, we're often dealing with systems and matrices which are not perfectly square. Many problems are not perfectly square. In reality, very few are. The rest of these problems fall into two general categories of systems which we call underdetermined and overdetermined systems. When it comes to solving $\vec{A}x=b$ for these problems, the question is ill-posed. Though these systems may have no solutions or infinitely many solutions, most software will still be able to solve the problem $\vec{A}x=b$, how is this done?

%TODO: More exposition on what it means for a system to be OD or UD.

\paragraph{Underdetermined systems $(m < n)$.}

These systems fundamentally have infinitely many solutions, so $\vec{A}x=b$ is instead posed as an optimization problem
\begin{equation}
  \min_x \norm{x}_2 \text{ such that } Ax = b.
\end{equation}
In this case, the minimization of the $L^2$ norm acts as a regularizer for our desired solution $x$.

\paragraph{Overdetermined systsems $(m>n)$.}

Due to the abundance of constraints, satisfying $\vec{A}x=b$ is technically impossible. In this case, we attempt to find the closest possible solution i.e.

\begin{equation}
  \min_x \norm{Ax + b} + \lambda \norm{x}_2.
\end{equation}

Here the $L^2$ norm acts as a regularizer for our solution $x$. We use the parameter $\lambda$ as a hyperparameter which determines the relative importance of the regularizer $\lambda$. Indeed, there are several different ways to do this regularization such as using the $L^1$ norm.

\begin{equation}
  \min_x \norm{Ax + b} + \lambda_1 \norm{x}_1 + \lambda_2 \norm{x}_2.
\end{equation}



\section{Linear Operators}%
\label{sec:linear_operators}

\paragraph{Linear operators.}%
\label{par:linear_operators}
%TODO: What should I include before these properties.

Linear operators are communtative and associative under addition.
\begin{enumerate}[(i)]
  \item \emph{Commutative} ($+$). $\vec{A} + \vec{B} = \vec{B} + \vec{A}$
  \item \emph{Associative} ($+$). $\vec{A} + (\vec{B} + \vec{C}) = (\vec{A} + \vec{B}) + \vec{C}$
  \item \emph{Distributive}. $\vec{A}(\vec{B} + \vec{C}) = \vec{AB} + \vec{BC}$
  \item \emph{Associative} ($\cdot$). $(\vec{AB})\vec{C} = \vec{A}(\vec{BC})$
\end{enumerate}

Though we have all these algebraic properties, it is important to know that multiplication is not commutative for matrices in general i.e. given any two linear operators, it is not necessary true that
\begin{equation}
  BA \neq AB. 
\end{equation}

\subsection{Matrix Fundamentals}
\paragraph{Matrices and vectors.}
For the majority of these notes, the linear operators we will work with will be complex matrices $\vec{A} \in \bbC^{n \times m}$ which operate on complex (column) vectors $\vec{x}\in\bbC^m$. We can illustrate these matrices and vectors with the following notation:
\begin{equation}
  \vec{A} = \begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1m} \\
    a_{21} & a_{22} &        & a_{2m} \\
    \vdots &        & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nm}
  \end{pmatrix} \text{ and } 
  \vec{x} = \begin{pmatrix}
  x_1 \\
  x_2 \\
  \vdots \\
  x_m
  \end{pmatrix}.
\end{equation}
where the numbers $a_{ij} \in \bbC$ and $x_i \in \bbC$ are called the entries of $\vec{A}$ and the elements of $\vec{x}$ respectively. Alternatively, we can represent this same matrix by its columns $\vec{a}_1, \ldots, \vec{a}_n$:

\begin{equation}
  \vec{A} = \begin{pmatrix}
    \vdots & \vdots &   & \vdots\\
    \vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_n \\
    \vdots & \vdots &  & \vdots
  \end{pmatrix}.
\end{equation}
Notice that the matrix $\vec{A}$ has $n$ rows and $m$ columns and that $\vec{x}$ is written so that it has $m$ rows. 

\paragraph{Addition on matrices and vectors.}
Addition between vectors happens element-wise. Similarly, addition between matrices occurs entry-wise. This means that addition is only well defined between matrices and vectors of the same size.

\paragraph{Scalar multiplication of vectors and matrices.}

\paragraph{Multiplying vectors by matrices.}

Given a matrix $\vec{A} \in \bbC^{n\times m}$ and a vector $\vec{x} \in \bbC^{m}$, we can make $\vec{A}$ act as a linear operator on $\vec{x}$. We can write the resulting vector $n$-vector $\vec{Ax}$ element-wise as:

\begin{equation}
  (\vec{Ax})_{i} = \sum_{j=1}^{n} a_{ij}x_j 
\end{equation}

We can also write this as a linear combination of the columns of $\vec{A}$.
\begin{equation}
  \vec{Ax} = \begin{pmatrix}
    \vdots \\
    x_1\vec{a}_1\\
    \vdots
  \end{pmatrix} + 
  \begin{pmatrix}
    \vdots \\
    x_2\vec{a}_2\\
    \vdots
  \end{pmatrix} + \cdots +
  \begin{pmatrix}
    \vdots \\
    x_n\vec{a}_n\\
    \vdots
  \end{pmatrix}.
\end{equation}

\paragraph{Matrix multiplication.}

\paragraph{Matrix inverses.}

\subsection{Norms and inner products}%
\label{sub:norms}

\paragraph{Defining the norm}
%TODO: Parahgraph describing norms and formal definition.

In short, a norm is just a way of quantifying distance. In particular, the two most interesting norms that we'll cover are the $L^2$ and $L^1$ norms.

\paragraph{$L^2$ norm}%
\label{par:_l_2_norm}
We can define the $L^2$ norm of a vector $\vec{x}$ as 
\begin{equation}
  \norm{\vec{x}}_2 = \sqrt{\abs{x_1}^2 + \abs{x_2}^2}
\end{equation}

Notice, this is the distance that we're used to in most geometric contexts.

\paragraph{$L^1$ norm}%
\label{par:_l_1_norm}

There are also various other norms such as the $L^1$ norm which we denote as $\norm{\cdot}_1$
\begin{equation}
  \norm{\vec{x}}_1 = \abs{x_1} + \abs{x_2}
\end{equation}

In applications, the $L^1$ norm tends to promote sparsity in solutions.

\paragraph{$L^p$ norms}%
\label{par:other_norms} 

In general, we can compute the $L^p$ norm of a vector $x$ as 
\begin{equation}
  \norm{\vec{x}}_p = (\abs{x_1}^p + \abs{x_2}^p)^{1/p}.
\end{equation}

There are additionally two \emph{special} $L$ norms which are the $L^\infty$ and $L^0$ norms.

%TODO: More info leading into this section.

\paragraph{$L^\infty$ norm}

\paragraph{$L^0$ norm}
%TODO: Talk about combinatorial difficuly associated with this.

There are also the $L^\infty$ and $L^0$ norms.
%TODO: Properties of norms
%TODO: Norm definition scalar mutliplication.

\paragraph{Inner products}%
\label{par:inner_products}

The inner product of two vectors $\vec{x}, \vec{y} \in \bbC^{m}$ is given by 
\begin{equation}
  \vec{x}^*\vec{y} = \sum_{i=1}^{m} \bar{x}_i y_i,
\end{equation}
where $\bar{z}$ denotes the \emph{complex conjugate of $z$}.   Notice that the inner product $\vec{x}^*\vec{y}$ is a scalar. The inner product is bilinear in the following sense. Suppose we have vectors $\vec{x}_1, \vec{x}_2, \vec{y}\in \bbC^m$ and two scalars $\alpha, \beta \in \bbC$. 

\begin{align}
  (\vec{x}_1 + \vec{x}_2)^*\vec{y} &= \vec{x}_1^*\vec{y} + \vec{x}_2^*\vec{y} \\
\vec{y}^*(\vec{x}_1 + \vec{x}_2) &= \vec{y}^*\vec{x}_1 + \vec{y}^*\vec{x}_2 \\
(\alpha \vec{x})^*(\beta \vec{y}) &= \bar{\alpha}\beta \vec{x}^*\vec{y}.
\end{align}

\subsection{Adjoint and Unitary Operators}%
\label{sub:adjoint_and_unitary_operators}

\paragraph{Adjoint}

We define the \emph{adjoint} of a matrix $\vec{A}$ to be the complex conjugate of its transpose $\vec{A}^*$. That is,

\begin{equation}
  \vec{A} = \begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1m} \\
    \vdots &        & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nm}
  \end{pmatrix} \implies
  \vec{A}^* = \begin{pmatrix}
    \bar{a}_{11} & \cdots & \bar{a}_{n1} \\
    \bar{a}_{12} &        & \bar{a}_{n2} \\
    \vdots       & \ddots & \vdots \\
    \bar{a}_{1m} & \cdots & \bar{a}_{nm}
  \end{pmatrix}. 
\end{equation}

Similarly, we note that the adjoint of a vector $\vec{x}$ is also the complex conjugate of its transpose. Therefore, our definition of the inner product simplifies to be multiplication of a vector and its adjoint. This why we use the same symbol $^*$ to denote both the inner product and adjoint.
%TODO: Expand upon the same symbol issue using an example.
\begin{exmp}
As a concrete example, consider the following matrix:

\begin{equation}
  \vec{A} = \begin{pmatrix}
    2 + i & 7 & i \\
    7i & 4 & 12 - i 
  \end{pmatrix}
\end{equation}

Under our definition, its adjoint is given by
\begin{equation}
  \vec{A}^* = \begin{pmatrix}
    2-i & -7i \\
    7   & 4   \\ 
    -i  & 12 + i
  \end{pmatrix}. 
\end{equation}
\end{exmp}

%TODO: AB)^* = B^*A^*

\paragraph{Hermitian Matrices}%
\label{par:hermitian_matrices}

In the case, that a matrix is its own adjoint i.e. $\vec{A} = \vec{A}^*$. We say that it is \emph{self-adjoint} or \emph{Hermitian}. All Hermitian matrices are square matrices.

%TODO: Properties of Hermitian matrices

\paragraph{Unitary Matrices}%
\label{par:unitary_matrices}
Another class of matrices related to the adjoint are the unitary matrices. A matrix $\vec{U}$ is said to be $\emph{unitary}$ if $\vec{U}^* \vec{U} = \vec{I}$. 

% TODO: Properties of Unitary Matrices

\subsection{Nullspaces and zero eigenvalues}%
\label{sub:nullspaces}

%TODO: Clean up this langauge and section.
Consider the problem $\vec{A}^*y = 0$ where $\vec{A}^*$ is the adjoint of the matrix $\vec{A}$. When does $\vec{A}x=b$ have a solution?
\begin{align}
  Ax \cdot y &= b \cdot y\\
  x\cdot A^*y &= b\cdot y \\
  b\cdot y &= 0
\end{align}

The Fredholm alternative is the statement that $b$ is not orthogonal to $y$, then $\vec{A}x=b$ has no solutions.
% TODO: Double check this. Flesh this out.

Suppose we have the problem $\vec{A}x=b$ and a vector $x_0$ with 0 eigenvalue $\vec{A}x_0 = 0$. Then we can generate solutions as any vector of the form
\begin{equation}
  x = \xi + \alpha x_0,
\end{equation}
where $\xi$ is a solution. The regularization process is an attempt to avoid this by minimizing across the vectors $x_0$ in the nullspace.

\section{Singular Value Decomposition}%
\label{sec:singular_value_decomposition}

%TODO: Supplement this section with notes from textbook

Here, we return to the singular value decomposition which is one of the most important matrix decompositions in the modern world. We'll begin with an example in 2-dimensions and then scale up the problem.

\begin{exmp}
Consider the following matrix and vector:
  \begin{equation}  
  \vec{A} = \begin{pmatrix}
    2 & 1\\
    -1 & 1
    \end{pmatrix} \quad \vec{x} = \begin{pmatrix}
    1 \\
    3
  \end{pmatrix}
  \end{equation}

  Using our standard linear algebra muscles, we can easily compute the product $\vec{Ax}$ as 

  \begin{equation}  
  \vec{Ax} = \begin{pmatrix}
    2 & 1\\
    -1 & 1
    \end{pmatrix}\begin{pmatrix}
    1 \\
    3
  \end{pmatrix} = \begin{pmatrix}
    5 \\
    2
  \end{pmatrix} = \vec{y}.
  \end{equation}
  Thinking about this geometrically, we can say that the matrix $\vec{A}$ rotated the vector $\vec{A}$ and then stretched it. In order to decompose this operation, we might ask: "How can we decompose the matrix $\vec{A}$ as a rotation and a stretching?". The former aspect is done with the standard rotation matrix $R_\theta$ which rotates a vector by an angle $\theta$

\begin{equation}
  R_\theta = \begin{pmatrix}
    \cos \theta & -\sin\theta\\
    \sin \theta & \cos\theta 
  \end{pmatrix}
\end{equation}

Similarly, the stretching can be accomplishing by multiplication by the matrix

\begin{equation}
\alpha\vec{I} = \begin{pmatrix}
  \alpha & 0 \\
         0 & \alpha
\end{pmatrix}.
\end{equation}
\end{exmp}

%TODO: IF we were to stretch each axis seperatingly, we would instead get a matrix like
%TODO: Applying a matrix like $\vec{A}$ assuming it has full rank takes your basis to a new orthonormal basis (Principal Axes) $\vec{u}_i$ with singular values which stretch them $sigma_i$.

Mathematically, we can write this as:
\begin{equation}
\vec{Av}_i = \sigma_1 \vec{u}_i.
\end{equation}
%TODO: Figure of this made in Julia

We can also stack these vectors $\vec{v}_i$ and $\vec{u}_i$ into a matrices of size $n \times n$, so that
%TODO: Are these sizes right?
\begin{equation}
  \vec{A} \begin{pmatrix}
    \vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_n
  \end{pmatrix} = \begin{pmatrix}
    \vec{u}_1 & \vec{u}_2 & \cdots & \vec{u}_n
    \end{pmatrix} \begin{pmatrix}
    \sigma_1 & & &  \\
             & \sigma_2 & & \\
             & & \ddots &  \\
             & & & \sigma_n
  \end{pmatrix}.
\end{equation}

More compactly, we can write this as 
\begin{equation}
  \vec{AV} = \vec{U\Sigma}.
\end{equation}

Since our vectors $\vec{v}_i$ map to another orthonormal basis $\vec{u_i}$, we can simply think of them as a kind of rotation. In fact the matrices, we construct $\vec{V}$ and $\vec{U}$ are both unitary matrices. Lastly, as we seen the matrix $\Sigma$ is a diagonal matrix. We can re-write this by taking advatange of the fact that $\vec{V}$ is unitary. Simply, right multiplying by the inverse of $\vec{V}$ shows that 
\begin{equation}
  A = \vec{U} \vec{\Sigma} \vec{V}^*.
\end{equation}
This is called the reduced singular value decomposition. 
%TODO: Go back and write the differnece between the reduced form and full form 

Usually, the columns of the singular value decomposition are ordered, so that $\sigma_1 \geq \sigma_2 \ldots$.

This decomposition is extremely robust in fact it is guaranteed to exist for any matrix $\vec{A}$.
\begin{thm}
  Every matriz $\vec{A} \in \bbC^{m\times n}$ has a singular value decomposition. Moreover,
  \begin{enumerate}[(i)]
    \item The singular values are uniquely determined and if the matrix $\vec{A}$ is square, then they are also distinct.
    \item The vectors $\vec{u}_i$ and $\vec{v}_i$ are also unique up to a complex sign.
  \end{enumerate}
\end{thm}

Suppose we've taken the SVD of $\vec{A}$, we can compute the following relationship between the eigenvalues and the SVD.
\begin{equation}
  \vec{A}^*\vec{A} = \vec{V}\vec{\Sigma}^2\vec{V}^*.
\end{equation}

Right multiplying by $\vec{V}$, we see that 

\begin{equation}
  \vec{A}^*\vec{A} \vec{V} = \vec{V}\vec{\Sigma}^2 \implies \lambda_j = \sigma_j^2.
\end{equation}

With a similar computation, we can show that
\begin{equation}
  \vec{A}\vec{A}^* \vec{U} = \vec{U} \vec{\Sigma}^2,
\end{equation}

which allows us to recover our matrix $\vec{U}$.

\section{Principal Component Analysis}%
\label{sec:principal_component_analysis}
%TODO: Do background research and uncomment below: 
% For real data, we can write the sample variance \sigma^2_{\vec{a}} = \frac{1}{n-1} \vec{a}^{T}\vec{a}, where $\vec{a}$ is a column vector.

% You always want to work with data that has mean 0 and unit variance when dealing with PCA.

% Covariance and variance matrix given by matrix C_x = \frac{1}{n-1} \vec{X}^{T} \vec{X} where $\vec{X}$ is our data matrix

% We want to take our the covariance and variance matrices and reduce this using the SVD to remove statistical redundacy in the data.

% Since \vec{X}\vec{X}^T is self-adjoint it has real eigenvalues with orthogonal eigenvecetors, therefore, we can write 
% \vec{X}\vec{X}^T = \vec{S}\vec{\Lambda} \vec{S}^*, where $\vec{S}$ is unitary and $\vec{\Lambda}$ is a diagonal matrix with eigenvalues of $\vec{XX}^T$ and $\vec{S}$ is the matrix of eigenvectors of $\vec{XX}^T$.
%Let's instead work with $\vec{Y} = \vec{S}^T\vec{X}$. C_Y = ... = \frac{1}{n-1}\Lambda TODO: Show this,

%TODO: IF we instead have the SVD  $X = \vec{U} \vec{\Sigma} \vec{V}^*$ and $Y = \vec{U}^* X$, then C_\vec{Y} = \frac{1}{n-1} \Sigma^2
%\subsection{PCA for Face Recognition}%
%\label{sub:pca_for_face_recognition}

%NOTE: X^TX if pics are in columns

\section{QR Decomposition}%
\label{sec:qr_decomposition}

The idea of the QR decomposition is to write it as a product of matrices $\vec{QR}$, where the matrix $\vec{Q}$ is unitary and $\vec{R}$ is upper triangular. In the case of $\vec{Ax} = \vec{b}$. We can decompose $\vec{A}$ using the $\vec{QR}$ decomposition, so that 

\begin{align}
  \vec{y} =\vec{Rx} &= \vec{Q}^*\vec{b}\\
  \vec{Qy} &= \vec{b}.
\end{align}
This reduces solving $\vec{Ax}=\vec{b}$ to a single matrix multiplication and back substitution.

Consider a matrix $\vec{A} \in \bbC^{m\times n}$ ($m>n$) with column vectors $\vec{a}_i$ which are linearly independent. These columns then form an $n$-dimensional basis
\begin{equation}
  < \vec{a}_1, \vec{a}_2, \ldots, \vec{a}_n>.
\end{equation} Therefore, given any vector of interest $\vec{x}\in \bbC^n$, we can write it as a linear combination of these basis vectors, so that 
\begin{equation}
  x = \sum_{j=1}^{n} \alpha_j \vec{a}_j.
\end{equation} We then have a linear system of equations
%TODO: Retrieve from lecture video

In order to simplify this, we'll attempt to work with an orthonormal basis instead. This would greatly simpliftly the math since if we have an orthonormal basis $\{\vec{q}_1, \ldots, \vec{q}_n \}$, then $\vec{\alpha}_i = \vec{q}^*_i \vec{x}$. Therefore,
\begin{equation}
  \vec{x} = \sum \alpha_j \vec{q}_j = \sum (\vec{q}_j ^* \vec{x}) \vec{q}_j.
\end{equation}

%TODO: Fill out details  
One such approach to finding these orthonormal vectors is to construct them from our original basis $<\vec{a}_1,\ldots, \vec{a}_n>$. This is the GS orthonormalization procedure.

%TODO: Fill out details connecting these paragraphs.
We have an iteration scheme $C\vec{q}_{2} = \vec{a}_{2} - (\vec{q}_{1}^* \vec{a}_{2})\vec{q}_{1}$ where $C$ is a normalization constant so that $\vec{q}_2$ has unit magnitude. This is the core idea behind Gram-Schmidt. By moving from the coordinate system of column space to an orthonormal system, we can simply the number of computations necessary for computations.
%TODO: Rewrite and link to the QR decomposition.
We have that

\begin{align}
  \vec{q}_1 &= \frac{\vec{a}_1}{r_{11}} \\
  \vec{q}_2 &= \frac{1}{r_{22}}\left(\vec{a}_2 - r_{12}\vec{q}_1 \right)\\
  \vec{q}_n &= \frac{1}{r_{nn}} \left( \vec{a}_n - \sum_{j=1}^{n-1} r_{jn}\vec{q}_j \right)\\
\end{align}

We can also work out the entries of $\vec{R}$ as
\begin{align}
  r_{ij} &= \vec{q}_i^*\vec{a}_j \\
  \abs{r_{jj}} &= \norm{ \vec{a}_j - \sum_{i=1}^{j-1} r_{ij}\vec{q}_i }_2.
\end{align}

%TODO: Figure out beginingg of lecture friday 16.

%TODO: Write out explicitly that $\vec{q}$ are the columns of $\vec{Q}$

\paragraph{Projectors}%
\label{par:projectors}


With the matrix $\vec{Q}$ at hand, we can write the vectors of $\vec{x}\in\bbC^m$ as 
\begin{equation}
  \vec{x} = \vec{r} + \sum_{j=1}^n (\vec{q}_j ^* \vec{x}) \vec{q}_j
\end{equation}
where $\vec{r}$ is orthogonal to the span of $\vec{Q}$ and the relationship
\begin{equation}
  \vec{x} \mapsto \sum_{j=1}^n (\vec{q}_j ^* \vec{x}) \vec{q}_j
\end{equation}
is the projector from $\vec{x}$ to $\text{range}(\vec{Q})$ which is $n$-dimensional. We could instead write this projector as 
\begin{equation}
  \vec{x} \mapsto \vec{QQ}^*\vec{x}
\end{equation}
taking advantage of the fact that the $\vec{q}_j$ form the columns of $\vec{Q}$. This gives us the opportunity to define projectors in general.

\begin{defn}
  A projector is a linear operator which satistifes that $\vec{P}^2 = \vec{P}$
\end{defn}

%TODO: Get all properties from lecture video
%TODO: Write out proposition statement
\begin{proof}
  \begin{align}
    \vec{P}[ \vec{y} = \vec{Px} ]
  \end{align}
\end{proof}

We define the complementary projectors as $\vec{I}- \vec{P}$. This is indeed a projecor itself
\begin{align}
  (\vec{I} - \vec{P})^2 &= \vec{I} -2 \vec{P} + \vec{P}^2\\
                        &= \vec{I} - \vec{P}.
\end{align}
We also see that $\text{range}(\vec{I} - \vec{P}) = \text{null}(\vec{P})$. Sometimes, we'll just denote this as 
\begin{equation}
  \vec{P}_\perp = \vec{I}-\vec{P}
\end{equation}
since the complementary projector is orthogonal to the original projector. There are plenty examples of projectors. For example, take a column vector $\vec{q}$, we can define the projector by $\vec{q}$ as
\begin{equation}
  \vec{P}_{\vec{q}} = \vec{q}\vec{q}^*. 
\end{equation}

%TODO: Tighten this langauge and explanation.
\paragraph{GS by projectors}%
\label{par:gs_by_projectors}

We can use this idea of projectors to implement our GS process. In short, the algorithm proceeds as follows for projectors $\vec{P}_1, \vec{P}_2, \ldots, \vec{P}_n$ and input vectors $\vec{a}_1, \ldots, \vec{a}_n$
\begin{align}
  \vec{q}_1 &= \frac{\vec{P}_1 \vec{a}_1}{\norm{\vec{P}_1 \vec{a}_1}} \\
  \vec{q}_2 &= \frac{\vec{P}_2 \vec{a}_2}{\norm{\vec{P}_2 \vec{a}_2}} \\
            &\vdots\\
  \vec{q}_n &= \frac{\vec{P}_n \vec{a}_n}{\norm{\vec{P}_n \vec{a}_n}}
\end{align}
Here, we define the projectors according to the following 
\begin{equation}
  \vec{P}_j = \vec{I} - \vec{Q}_{j-1}\vec{Q}_{j-1}^*,
\end{equation}
where $\vec{Q}_{j-1}$ is just the matrix with columns $\vec{q}_i$ for $i = 1, \ldots, j-1$. Another way of writing this as 
\begin{equation}
  \vec{P}_j = \vec{P}_{\perp\vec{q}_{j-1}}\vec{P}_{\perp\vec{q}_{j-2}} \cdots \vec{P}_{\perp\vec{q}_{1}},
\end{equation}
where $\vec{P}_1 = \vec{I}$. As discussed before, the multiplication $\vec{P}_{\perp\vec{q}_i}$ ensures that the next vector $\vec{q}_j$ is orthogonal to $\vec{q}_i$. This iteration scheme is stable comparted to the one discussed initially.
\end{document}
