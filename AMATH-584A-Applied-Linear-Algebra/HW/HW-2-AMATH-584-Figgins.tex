% !TeX options=--shell-escape
%Preamble
\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsrefs}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage[mathcal]{eucal} %% changes meaning of \mathcal
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{verbatim} %% includes comment environment
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\crefformat{equation}{~(#2#1#3)}
\usepackage{caption, subcaption}
\usepackage{graphicx}
\usepackage{fullpage} %%smaller margins
\usepackage[all,arc]{xy}
\usepackage{mathrsfs}
\usepackage{minted}

\hypersetup{
    linktoc=all,     % set to all if you want both sections and subsections linked
}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\setlength{\headheight}{16pt}

\linespread{1.0}

\pagestyle{fancy}
\lhead{\Name}
\chead{\hwClass: \hwTitle}
\rhead{\hwDueDate}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%% Title Info
\newcommand{\hwTitle}{HW \# 2}
\newcommand{\hwDueDate}{October 16, 2020}
\newcommand{\hwClass}{AMATH 584}
\newcommand{\hwClassTime}{}
\newcommand{\hwClassInstructor}{}
\newcommand{\Name}{\textbf{Marlin Figgins}}


%% MATH MACROS
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\abs}[1]{ \left| #1 \right| }
\newcommand{\diff}[2]{\frac{d #1}{d #2}}
\newcommand{\infsum}[1]{\sum_{#1}^{\infty}}
\newcommand{\norm}[1]{ \left|\left| #1 \right|\right| }
\newcommand{\eval}[1]{ \left. #1 \right| }
\newcommand{\Expect}[1]{\mathbb{E}\left[#1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[#1 \right]}
\renewcommand{\vec}[1]{\mathbf{#1}}

\renewcommand{\phi}{\varphi}
\renewcommand{\emptyset}{\O}

%--------Theorem Environments--------
%theoremstyle{plain} --- defaultx
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}

% Environments for answers and solutions
\newtheorem{exer}{Exercise}
\newtheorem{sol}{Solution}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\makeatletter
\let\c@equation\c@thm
\makeatother

\begin{document}
\subsection*{Yale Faces B}%
\label{sub:yale_faces_b}

The code used for the following questions and figures was done in Julia and is attached to the end of this document. We start with an analysis of the cropped images.
\begin{exer}
Do an SVD analysis of the images (where each image is reshaped into a column vector and each column is a new image).
\end{exer}
\begin{sol}
    I separated the data into a train-test set with 70\% training and 30\% test. Each training image was flatten into a column vector $\vec{x}_j$ and then combined into the training matrix $\vec{X}$. I then did the SVD on training data matrix $\vec{X}$.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/hw-2-cropped-singular-values.pdf}
    \caption{The singular values of the SVD of the training data matrix $\vec{X}$ for the cropped data set.}%
    \label{fig:cropped_spectrum}
\end{figure}

I also did this same analysis on the uncropped and unaligned data but with train-test split 80-20.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/hw-2-uncropped-singular-values.pdf}
    \caption{The singular values of the SVD of the training data matrix $\vec{X}$ for the uncroppped data set.}%
    \label{fig:uncropped_spectrum}
\end{figure}

\end{sol}

\begin{exer}
    What is the interpretation of the $\vec{U}$, $\vec{\Sigma}$ and $\vec{V}$ matrices? (Plot the first few reshaped columns of $\vec{U}$)
\end{exer}
\begin{sol}
    Personally, I like to think of the SVD in the terms of the following equation.
    \begin{equation}
        \vec{XV} = \vec{U}\vec{\Sigma}.
    \end{equation}
    In this interpretation, $\vec{V}$ is a change of coordinates of $\vec{X}$, so that $\vec{X}$ can be reduced to rotations and scalings. This way, since each column of $\vec{X}$ corresponds to a face, we can think about the matrix $\vec{U}$ as describing a basis in a new vector space of faces. We can plot its column vectors $\vec{u}_j$ as faces as in\cref{fig:cropped_eigenfaces} and \cref{fig:uncropped_eigenfaces} since they have the same shape as our input images. The singular values $\sigma_j$ are a measure of the the weights of the corresponding column $\vec{u}_j$.
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-cropped-eigenfaces.pdf}
        \caption{The first 20 column vectors in $\vec{U}$ corresponding the highest singular values $\sigma_j$ for the cropped faces dataset. }
        \label{fig:cropped_eigenfaces}
    \end{figure}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-uncropped-eigenfaces.pdf}
        \caption{The first 20 column vectors in $\vec{U}$ corresponding the highest singular values $\sigma_j$ for the uncropped faces dataset. }
        \label{fig:uncropped_eigenfaces}
    \end{figure}
\end{sol}

\newpage

\begin{exer}
What does the singular value spectrum look like and how many modes are necessary for good image
reconstructions using the PCA basis? (i.e. what is the rank $r$ of the face space?)
\end{exer}
\begin{sol}
    The singular value spectrum for the cropped faces is plotted in \ref{fig:cropped_spectrum} and \ref{fig:uncropped_spectrum}. With both data sets, we can use the test set to analyze how rank is need to be able to sucessfully recreate a face. We can see this tested in \cref{fig:cropped-rank-approximation} and \cref{fig:uncropped-rank-approximation}.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-cropped-rank-approximation.pdf}
        \caption{Various rank approximations for a test image on the cropped photos.}%
        \label{fig:cropped-rank-approximation}
    \end{figure}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-uncropped-rank-approximation.pdf}
        \caption{Various rank approximations for a test image on the uncropped photos.}%
        \label{fig:uncropped-rank-approximation}
    \end{figure}

    I also experimented with determining the cutoff necessary for a good reconstruction using percentage energy to cutoff. That is, I wanted to find the minimal rank $r$ so that
    \begin{equation}
        \sum_{i=1}^r \sigma_i < \lambda \sum_{i=1}^{n} \sigma_i
    \end{equation}
    for some threshold percentage $\lambda\in(0,1)$. I visualize where these thresholds fall for the cropped data in \cref{fig:cropped-percent-energy-thresholds}. This method allowed me to generate percent energy reconstructions as in \cref{fig:cropped-percent-energy-approximation} and \cref{fig:uncropped-percent-energy-approximation}.
    \begin{figure}[]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-cropped-percent-energy-thresholds.pdf}
        \caption{Percent energy threshold for cropped data at various cutoffs $\lambda$. Each vertical line is at the rank at which the threshold is met.}%
        \label{fig:cropped-percent-energy-thresholds}
    \end{figure}
    \begin{figure}[]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-uncropped-percent-energy-thresholds.pdf}
        \caption{Percent energy threshold for uncropped data at various cutoffs $\lambda$. Each vertical line is at the rank at which the threshold is met.}%
        \label{fig:uncropped-percent-energy-thresholds}
    \end{figure}

    \begin{figure}[]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-cropped-percent-energy-approximation.pdf}
        \caption{Approximation based on percent energy for cropped data set}%
        \label{fig:cropped-percent-energy-approximation}
    \end{figure}
    \begin{figure}[]
        \centering
        \includegraphics[width=0.8\linewidth]{../figures/hw-2-uncropped-percent-energy-approximation.pdf}
        \caption{Approximation based on percent energy for uncropped data set}%
        \label{fig:uncropped-percent-energy-approximation}
    \end{figure}
\end{sol}


\begin{exer}
Compare the difference between the cropped (and aligned) versus uncropped images in terms of singular
value decay and reconstruction capabilities.
\end{exer}

\begin{sol}
    We can compare the rate of singular value decay by using the thresholding discussed in the previous exercise. Comparing figures \cref{fig:cropped-percent-energy-thresholds} and \cref{fig:uncropped-percent-energy-thresholds}, we see that the uncropped data has a lower increase in energy. Additionally, from our attempts at reconstruction, we can see that the unaligned data performs poorly because the variance in the pixels for each image is not attributable to differences in the faces themselves but mostly due to misalignment of the faces. This misalignment is also seen in the uncropped eigenfaces in \cref{fig:uncropped_eigenfaces}. This leads to a poor fit as the SVD cannot pick up on the underlying structure of the faces. 
\end{sol}
\newpage

\subsection*{Theorems}%
\label{sub:theorems}

\begin{exer}
    The non-zero singular values of $\vec{A}$ are the square roots of the non-zero eigenvalue of $\vec{A}\vec{A}^*$ or $\vec{A}^*\vec{A}$.
\end{exer}
\begin{sol}
    Let $\vec{A} = \vec{U\Sigma V}^*$. Then we can write 
    \begin{align}
        \vec{A A}^* &= \vec{U}\vec{\Sigma}\vec{\Sigma}^*\vec{U}^{-1},\\
        \vec{A}^*\vec{A}  &= \vec{V}\vec{\Sigma}\vec{\Sigma}^*\vec{V}^{-1}
    \end{align}
    using the fact that $(\vec{AB})^* = \vec{B}^*\vec{A}^*$ and that both $\vec{U}$ and $\vec{V}$ are unitary. Notice that that both $\vec{\Sigma}\vec{\Sigma}^*$ and $\vec{\Sigma}^*\vec{\Sigma}$ are both diagonal with entries ($\sigma_1^2,\sigma_2^2, \ldots)$ which are the square of the non-zero singlar values $\sigma_i$. Looking at the above equations, we additionally see that $\vec{A}\vec{A}^*$ and $\vec{A}^*\vec{A}$ are diagonalizable. Therefore, the diagonal entries of the inner matrices $\vec{\Sigma}\vec{\Sigma}^*$ and $\vec{\Sigma}^*\vec{\Sigma}$ give their eigenvalues $\lambda_i$. This gives us that
    \begin{equation}
        \lambda_i = \sigma_i^2 \implies \sigma_i = \sqrt{\lambda_i}.
    \end{equation}
\end{sol}

\begin{exer}
    If $\vec{A}=\vec{A}^*$, then the singlar values are the absolute values of the eigenvalues of $\vec{A}$.
\end{exer}

\begin{sol}
    Since the singular values are the square roots of the non-zero eigenvalues of $\vec{A}^*\vec{A}$ and $\vec{A}$ is Hermitian, we have 
    \begin{equation}
        \vec{A}^2=\vec{A}^*\vec{A}.
    \end{equation}
Therefore, the singular values of singular values are simple the square root of the eigenvalues of $\vec{A}^2$. Since $\vec{A}$ is diagonalizable, we have that
\begin{equation}
    \vec{A}^2 = (\vec{P}\vec{\Lambda}\vec{P}^{-1})^2 = \vec{P}\vec{\Lambda}^2\vec{P}.
\end{equation}
Therefore, the eigenvalues $\vec{A}^2$ are simply the square of the eigenvalues of $\vec{A}$. This means that each sngular value is given by
\begin{equation}
    \sigma_i = \sqrt{ \lambda_i^2 } = \abs{\lambda_i},
\end{equation}
where $\lambda_i$ is the $i$-th eigenvalue of $\vec{A}$.
\end{sol}

\begin{exer}
    Given that the determinant of a unitary matrix is 1, show that $\abs{\det (\vec{A})} = \prod \sigma_i$.
\end{exer}

\begin{sol}
    Writing the singular value decomposition of $\vec{A}$ as $\vec{A} = \vec{U}\vec{\Sigma}\vec{V}^*$, we can see that

    \begin{align}
    \abs{\det(\vec{A})} = \abs{\det(\vec{U})} \abs{\det(\vec{\Sigma})} \abs{\det(\vec{V}^*)}.   
    \end{align}

Since both $\vec{U}$ and $\vec{V}$ are unitary, this reduces to
\begin{equation}
    \abs{\det(\vec{A})} = \abs{\det(\vec{\Sigma})}. 
\end{equation}

Since $\vec{\Sigma}$ is diagonal, its determinant is simply the product of its diagonal entries which are the singular values $\sigma_i$. Therefore,
\begin{equation}
    \abs{\det(\vec{A})} = \prod \sigma_i. 
\end{equation}
\end{sol}

\newpage
\section*{Code used}%
\label{sec:code_used}

\inputminted{julia}{HW-2-Code-Figgins.jl}
\end{document}
