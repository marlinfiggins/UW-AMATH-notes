%Preamble
\documentclass[12pt]{article}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsrefs}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage[mathcal]{eucal} %% changes meaning of \mathcal
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{verbatim} %% includes comment environment
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\crefformat{equation}{~(#2#1#3)}
\usepackage{caption, subcaption}
\usepackage{graphicx}
\usepackage{fullpage} %%smaller margins
\usepackage[all,arc]{xy}
\usepackage{mathrsfs}

\hypersetup{
    linktoc=all,     % set to all if you want both sections and subsections linked
}

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\setlength{\headheight}{16pt}

\linespread{1.1}

\pagestyle{fancy}
\lhead{\Name}
\chead{\hwTitle}
\rhead{\hwClass}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%% Title Info
\newcommand{\hwTitle}{HW \# 4}
\newcommand{\hwDueDate}{Feb 12, 2021}
\newcommand{\hwClass}{AMATH 562}
\newcommand{\hwClassTime}{}
\newcommand{\hwClassInstructor}{}
\newcommand{\Name}{\textbf{Marlin Figgins}}


%% MATH MACROS
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\abs}[1]{ \left| #1 \right| }
\newcommand{\diff}[2]{\frac{d #1}{d #2}}
\newcommand{\infsum}[1]{\sum_{#1}^{\infty}}
\newcommand{\norm}[1]{ \left|\left| #1 \right|\right| }
\newcommand{\eval}[1]{ \left. #1 \right| }
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}

\renewcommand{\phi}{\varphi}
\renewcommand{\emptyset}{\O}
\let\vec\mathbf
\def\mA{{\bf A}}
\def\vT{{\bf T}}
\def\vx{{\bf x}}
\def\vy{{\bf y}}
\def\rd{{\rm d}}

%--------Theorem Environments--------
%theoremstyle{plain} --- defaultx
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}

% Environments for answers and solutions
\newtheorem{exer}{Exercise}
\newtheorem{sol}{Solution}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\makeatletter
\let\c@equation\c@thm
\makeatother

\def\mA{{\bf A}}
\def\vT{{\bf T}}
\def\vx{{\bf x}}
\def\vy{{\bf y}}
\def\d{{\rm d}}
\def\vnu{\mbox{\boldmath$\nu$}}
\def\vpi{\mbox{\boldmath$\pi$}}

\begin{document}

\begin{exer}
    Compute $\d (W_{t}^{4})$. Write $W_{T}^{4}$ as an integral with respect to $W$ plus an integral with respect
to $t$. Use this representation of $W_{T}^{4}$ to show that $\Expect W_{T}^{4} = 3T^{2}$ . Compute $\Expect W_{T}^{6}$ using the same technique.
\end{exer}

\begin{sol}
    Using Thm 8.2.1. (MLN) with $f(x) = x^{4}$,
\begin{equation*}
    W_{T}^{4} = \int_{0}^{T} 4 W_{t}^{3} \d W_{t} + \frac{1}{2} \int_{0}^{T} 12 W_{t}^{2} \d t.
\end{equation*}
We can write this in differential form as
\begin{equation*}
    \d ( W_{t}^{4} ) = 6 W_{t}^{2} \d t +  4 W_{t}^{3} \d W_{t}.  
\end{equation*}
We can solve for the expectation of $W_{T}^{4}$ as
\begin{align*}
    \Expect[ W_{T}^{4} ] = 4\Expect \left[\int_{0}^{T} W_{t}^{3} \d W_{t}  \right] + 6 \Expect \left[\int_{0}^{T} W_{t}^{2} \d t \right].
\end{align*}
The left most integral defines a martigale so that its expection is given by $I_{0} = 0$, we then have
\begin{align*}
    \Expect[ W_{T}^{4} ] &= 6 \Expect \left[\int_{0}^{T} W_{t}^{2} \d t \right]\\
                         &= 6 \int_{0}^{T} \Expect[ W_{t}^{2} ] \d t\\
                         &= 6 \int_{0}^{T} t \d t = 3 T^{2},
\end{align*}
where we've interchanged the order of integration using Fubini and used that the variance of the standard Brownian motion is $\Expect[ W_{t}^{2} ] = t$. We'll now repeat this method to solve for $\Expect[W_{T}^{6}]$. We have that
\begin{equation*}
    \Expect[ W_{T}^{6} ] = \Expect \left[ \int_{0}^{T} 6 W_{t}^{5} \d W_{t} \right] + \Expect \left[ \frac{1}{2} \int_{0}^{T} 30 W_{t}^{4} \d t \right].
\end{equation*}
Once again, the first integral is a martingale and has expectation 0. We can also switch the order of integral using Fubini's theorem so that
\begin{align*}
    \Expect[ W_{T}^{6} ] &= 15 \int_{0}^{T} \Expect \left[  W_{t}^{4}\right]  \d t\\ 
                         &= 15 \int_{0}^{T} 3t^{2} \d t \\
                         &= 15 T^{3}.
\end{align*}
\end{sol}

\newpage

\begin{exer}
Find an explicit expression for $Y_{T}$ where
\begin{equation*}
\d Y_{t} = r \d t + \alpha Y_{t} \d W_{t}
\end{equation*}
Hint: compute $\d (Y_t Z_t)$ where $Z_t \coloneqq \exp(- \alpha W_t + \frac{1}{2} \alpha^2 t)$.
\end{exer}

\begin{sol}
Using the product rule, we have that
\begin{align*}
    \d (Y_{t} Z_{t}) = Z_{t} \d Y_{t} + Y_{t} \d Z_{t} + \d [Y, Z]_{t}.
\end{align*}
With the choice of $Z_{t}$, we have that
\begin{align*}
\d Z_{t} =\alpha^{2} Z_{t} \d t - \alpha Z_{t} \d W_{t}.
\end{align*}
Putting this together, we compute
\begin{align*}
    Z_{t} \d Y_{t} &= r Z_{t} \d t + \alpha Y_{t} Z_{t} \d W_{t}\\
    Y_{t} \d Z_{t} &= \alpha^{2} Z_{t} Y_{t} \d t - \alpha Z_{t} Y_{t} \d W_{t}.
\end{align*}

We can additionally compute
\begin{align*}
    \d[Y, Z]_{t} = (\alpha Y_{t} \d W_{t} )( - \alpha Z_{t} \d W_{t}) =  - \alpha^{2}  Y_{t} Z_{t} \d t.
\end{align*}

We can now write
\begin{align*}
    \d (Y_{t}Z_{t}) = r Z_{t} \d t\\
    Y_{T} Z_{T} = Y_{0} Z_{0} + \int_{0}^{T} r Z_{t} \d t
\end{align*}
Noting that $Z_{0} = 1$, we have that
\begin{align*}
    Y_{T} &= \frac{1}{Z_{T}} \left(Y_{0} + \int_{0}^{T} r \exp(- \alpha W_{t} + \frac{1}{2} \alpha^{2} t) \d t \right)\\
          &= \exp(\alpha W_{T} - \frac{1}{2} \alpha^{2}T)\left(Y_{0} + \int_{0}^{T} r \exp(- \alpha W_{t} + \frac{1}{2} \alpha^{2} t) \d t \right)
\end{align*}

\end{sol}
\newpage

\begin{exer}
Suppose $X$, $\Delta$ and $\Pi$ are given by
\begin{equation*}
    \d X_{t} = \sigma X_{t} \d W_{t}, \quad \Delta_{t} = \frac{\partial f}{\partial x}(t, X_{t}), \quad \Pi_{t} = X_{t} \Delta_{t},
\end{equation*}
where $f$ is some smooth function. Show that if $f$ satisfies 
\begin{equation*}
    \left( \frac{\partial }{\partial t} + \frac{1}{2} \sigma^{2} x^{2} \frac{\partial^{2} }{\partial x^{2}}   \right ) f(t, x) = 0.
\end{equation*}
for all $(t,x)$, then $\Pi$ is a martingale with respect to a filtration $\calF$ for $W$.
\end{exer}

\begin{sol}
Let $g(t,x) = \frac{\partial f}{\partial x}(t, x)$, we then have that
\begin{align*}
    \d \Delta_{t} &= \d g(t, X_{t}) = \partial_{t} g(t, X_{t}) \d t + \partial_{x} g(t, X_{t})dX_{t} + \frac{1}{2} \partial_{xx} g(t, X_{t}) \d [X, X]_{t}\\
                  &= \left( \frac{\partial}{\partial t } + \frac{1}{2} \sigma^{2} X^{2}_{t} \frac{\partial^{2} }{\partial x^{2}}\right) g(t, x) \d t + \sigma X_{t} \frac{\partial }{\partial x} g(t, X_{t})  \d W_{t}.
\end{align*}
Using product rule, we write that
\begin{align*}
    \d \Pi_{t} = \d(X_{t} \Delta_{t})=\Delta_{t} \d X_{t} + X_{t} \d \Delta_{t} + \d [\Delta, X]_{t}.
\end{align*}
We can compute the three terms as
\begin{align*}
    \Delta_{t} \d X_{t} &= (\sigma X_{t}) g(t, X_{t}) \d W_{t}\\
    X_{t} \d \Delta_{t} &= \left( X_{t} \frac{\partial}{\partial t } + \frac{1}{2} \sigma^{2} X^{3}_{t} \frac{\partial^{2} }{\partial x^{2}}\right) g(t, x)  \d t + \sigma X_{t}^{2} \frac{\partial }{\partial x}   g(t, X_{t}) \d W_{t}\\
    \d [\Delta, X]_{t} &= \sigma^{2} X_{t}^{2} \frac{\partial}{\partial x} g(t,X) \d t
\end{align*}
Therfore, we can write $\Pi_{t}$ as
\begin{align*}
    \Pi_{T} - \Pi_{0} &= \int_{0}^{T} \left( X_{t} \frac{\partial}{\partial t } + \frac{1}{2} \sigma^{2} X^{3}_{t} \frac{\partial^{2} }{\partial x^{2}} + \sigma^{2} X_{t}^{2} \frac{\partial}{\partial x} \right) g(t, X_{t}) \d t + \int_{0}^{T} \sigma X_{t}^{2} \frac{\partial }{\partial x}   g(t, X_{t}) \d W_{t}\\
                      &= \int_{0}^{T} X_{t} \left(\frac{\partial^{2}}{\partial t \partial x} + \frac{1}{2}\sigma X_{t}^{2} \frac{\partial^{3} }{\partial x^{3}} + \sigma^{2} X_{t} \frac{\partial^{2} }{\partial x^{2}}  \right)f(t,X_{t} ) \d t  + \int_{0}^{T} \sigma X_{t}^{2} \frac{\partial }{\partial x}   g(t, X_{t}) \d W_{t}.
\end{align*}
Differentiating the condition to us with respect to $f$ shows that
\begin{align*}
        \frac{\partial }{\partial x} \left( \frac{\partial }{\partial t} + \frac{1}{2} \sigma^{2} x^{2} \frac{\partial^{2} }{\partial x^{2}}  \right) = \left(\frac{\partial^{2}}{\partial t \partial x} + \frac{1}{2}\sigma x^{2} \frac{\partial^{3} }{\partial x^{3}} + \sigma^{2} x \frac{\partial^{2} }{\partial x^{2}}  \right)f(t, x) = 0
\end{align*}
Therefore, 
\begin{align*}
\Pi_{T} - \Pi_{0} &= \int_{0}^{T} \sigma X_{t}^{2} \frac{\partial }{\partial x}   g(t, X_{t}) \d W_{t}.
\end{align*}

As this is an Ito integral with respect to $W_{t}$ it is a martingale with respect to $\calF_{t}$
\end{sol}

\newpage

\begin{exer}
Suppose $X$ is given by
\begin{equation*}
    \d X_{t} = \mu(t, X_{t}) \d t + \sigma(t, X_{t}) \d W_{t}.
\end{equation*}
For any smooth function $f$ define

\begin{equation*}
    M_{t}^{f} \coloneqq f(t, X_{t}) - f(0, X_{0}) - \int_{0}^{t} \left( \frac{\partial }{\partial s} + \mu(s, X_{s}) \frac{\partial }{\partial x} + \frac{1}{2} \sigma^{2}(s, X_{s}) \frac{\partial^{2} }{\partial x^{2}}     \right ) f(s, X_{s})ds.
\end{equation*}

Show that $ M^{f} $ is a martingale with respect to a filtration $\calF$ for $W$.
\end{exer}

\begin{sol}
We begin by working with
\begin{align*}
    \d f(t, X_{t}) &= \frac{\partial f}{\partial t}(t, X_{t}) \d t + \frac{\partial f}{\partial x} (t, X_{t}) \d X_{t}  + \frac{1}{2} \frac{\partial^{2} f}{\partial x^{2}} (t, X_{t}) \d[X, X]_{t}\\
                   &= \left( \frac{\partial}{\partial t} + \mu(t, X_{t}) \frac{\partial }{\partial x} + \frac{1}{2} \sigma^{2}(t, X_{t}) \frac{\partial^{2} }{\partial x^{2}}  \right)f(t, X_{t}) \d t\\
                   &+ \sigma(t, X_{t})  \frac{\partial }{\partial x} f(t, X_{t}) \d W_{t}.
\end{align*}

We can rewrite this in integral form as
\begin{align*}
    f(t,X_{t}) - f(0, X_{0}) &= \int_{0}^{t} \left( \frac{\partial }{\partial s} + \mu(s, X_{s}) \frac{\partial }{\partial x} + \frac{1}{2} \sigma^{2}(s, X_{s}) \frac{\partial^{2} }{\partial x^{2}} \right ) f(s, X_{s})ds\\
    &+ \int_{0}^{t} \sigma(t, X_{t})  \frac{\partial }{\partial x} f(t, X_{t}) \d W_{t}.
\end{align*}
This allows us to rewrite $M_{t}^{f}$ as
\begin{align*}
    M_{t}^{f} = \int_{0}^{t} \sigma(t, X_{t}) \frac{\partial}{\partial x} f(t, X_{t}) \d W_{t}.
\end{align*}
As this is an Ito integral with respect to $W_{t}$ it is a martingale with respect to $\calF_{t}$
\end{sol}

\newpage

\begin{exer}
    Let $X = (X_{t})_{0\leq t\leq T}$ be an OU process on a probability space $(\Omega, \calF, \Prob)$
    \begin{equation*}
        \d X_{t} = K(\theta - X_{t}) \d t + \sigma \d W_{t},
    \end{equation*}
where $\{ W_{t} \}_{ 0\leq t \leq T}$ is a Brownian motion under probability $\Prob$. Then we can define a new probability measure $\tilde{\Prob}$ such that the process  $\tilde{W} = (\tilde{W}_{t})_{0\leq t \leq T}$ is a Brownian motion under $\tilde{\Prob}$. Then the OU process  $X  = (X_{t})_{0\leq t \leq T}$ on the new probability space $(\Omega, \calF, \tilde{\Prob})$ will be 
    \begin{align*}
        \d X_{t} = K(\theta^{*} - X_{t}) \d t   + \sigma \d \tilde{W}_{t}.
    \end{align*}
\end{exer}

\begin{sol}
    We'll seek to write $\tilde{W}_{t}$ in temrs of $W_{t}$. Writing both representations of $X_{t}$ in integral form, we have that
    \begin{align*}
        X_{T} - X_{0} &= \int_{0}^{T} K(\theta - X_{t}) \d t + \int_{0}^{T} \sigma \d W_{t}\\ 
                      &= \int_{0}^{T} K(\theta^{*} - X_{t}) \d t + \int_{0}^{T} \sigma \d \tilde{W}_{t}.
    \end{align*}
    We can rewrite this so that
    \begin{align*}
        \int_{0}^{T} \d \tilde{W}_{t} &= \frac{1}{\sigma}\int_{0}^{T} K(\theta - X_{t}) - K(\theta^{*} - X_{t}) \d t + \int_{0}^{T} \d W_{t}\\
                                      &= \frac{1}{\sigma} \int_{0}^{T} K (\theta - \theta^{*})\d t + \int_{0}^{T} \d W_{t}.
    \end{align*}
    Alternatively, in differential form this is
    \begin{equation*}
        \d \tilde{W}_{t} = \frac{1}{\sigma}  K (\theta - \theta^{*})\d t + \d W_{t}.
    \end{equation*}
    Using that $\tilde{W}_{t}$ is a Brownian motion under $\tilde{\Prob}$ and $W_{t}$ is a Brownian motion under $\Prob$, we can then use the Girsanov theorem (Thm 8.5.5 in MLN) to see that 
    \begin{equation*}
        \frac{\d\tilde{\Prob}}{\d \Prob} = \exp\left( - \int_{0}^{T} \frac{1}{2} \left( \frac{K}{\sigma}  (\theta - \theta^{*})  \right )^{2} \d t -  \int_{0}^{T} \frac{K}{\sigma}  (\theta - \theta^{*}) \d W_{t} \right)
    \end{equation*}
\end{sol}
\end{document}
